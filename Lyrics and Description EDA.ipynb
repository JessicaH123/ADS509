{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics\n",
    "Jessica Hin  \n",
    "008515095  \n",
    "May 20th, 2024  \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b05875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the libraries I need\n",
    "#!pip install emoji\n",
    "#!pip install collections\n",
    "#!pip install nltk\n",
    "#!pip install string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"C:/Users/jessh/Documents/ADS509/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder =  \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5 , verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    flat_tokens = [token for token in tokens if not isinstance(token,float)]\n",
    "    #flat_tokens = [token for sublist in tokens if isinstance(sublist, list) for token in sublist if not isinstance(token, float)]\n",
    "    num_tokens = len(flat_tokens) # number of tokens\n",
    "    num_unique_tokens = len(set(flat_tokens)) # number of unique tokens\n",
    "    lexical_diversity = num_unique_tokens/num_tokens  # lexical diversity\n",
    "    num_characters = sum(len(token) for token in flat_tokens) # number of characters\n",
    "    most_common_tokens = Counter(flat_tokens).most_common(num_tokens)[0:5] # most common tokens\n",
    "   \n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "        \n",
    "    print(f\"The five most common tokens are {most_common_tokens} in the data.\")      \n",
    "    \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The five most common tokens are [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)] in the data.\n",
      "The five most common tokens are [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)] in the data.\n",
      "The five most common tokens are [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)] in the data.\n",
      "The five most common tokens are [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)] in the data.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: It is beneficial to use assertion statements to test, debug, and detect errors. In this case, it is used to check if your user defined function works as expected. Based on the the text above, the assertions statements check if the number of tokens, number of unique tokens, characters, and lexical diversity are correct. If it does, then there should be a \"AssertionError\" flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883f53a",
   "metadata": {},
   "source": [
    "### Reading in Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa95736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined function for removing the www.azlyrics.com portion of the filenames\n",
    "def edit_filename(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "\n",
    "    # drop the http or https and the html\n",
    "    name = link\n",
    "    name = re.sub(r'^www_azlyrics_com_', '', name)\n",
    "    \n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516d972",
   "metadata": {},
   "source": [
    "This next cell of code was just renaming the files within each artist's folder. The for loop was ran twice for each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13659e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files renamed successfully!\n"
     ]
    }
   ],
   "source": [
    "# renaming all the files in each artist folders to remove the www_azlyrics_com_ portion in the filename\n",
    "dualipa_folder_path = data_location + lyrics_folder + \"dualipa\"\n",
    "hozier_folder_path = data_location + lyrics_folder + \"hozier\"\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(hozier_folder_path)\n",
    "\n",
    "# Iterate through each file and rename them\n",
    "for filename in files:\n",
    "    # Construct the full old and new file paths\n",
    "    old_filepath = os.path.join(hozier_folder_path, filename)\n",
    "    new_filename = old_filepath.replace(\"www_azlyrics_com_\", \"\") \n",
    "    new_filepath = os.path.join(hozier_folder_path, new_filename)\n",
    "    \n",
    "    try:\n",
    "        # Rename the file\n",
    "        os.rename(old_filepath, new_filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to rename '{filename}': {e}\")\n",
    "\n",
    "print(\"Files renamed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f0764",
   "metadata": {},
   "source": [
    "We have to get to the folder of each artist and then pull the text files and store them into a dataframe. The folder path that contained both artist was defined first, then within the first for loop, they got the path to each artist and their corresponding lyric files. The result is a dataframe with \"Filename\" as one column and another column \"File Content\" that contains the whole text file content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a260ec1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>File Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dualipa_badtogether.txt</td>\n",
       "      <td>\"Bad Together\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dualipa_begging.txt</td>\n",
       "      <td>\"Begging\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n\\n\\n\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dualipa_betheone.txt</td>\n",
       "      <td>\"Be The One\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dualipa_blowyourmindmwah.txt</td>\n",
       "      <td>\"Blow Your Mind (Mwah)\" lyrics\\n\\nDua Lipa Lyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dualipa_dreams.txt</td>\n",
       "      <td>\"Dreams\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n\\n\\n\"D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Filename  \\\n",
       "0       dualipa_badtogether.txt   \n",
       "1           dualipa_begging.txt   \n",
       "2          dualipa_betheone.txt   \n",
       "3  dualipa_blowyourmindmwah.txt   \n",
       "4            dualipa_dreams.txt   \n",
       "\n",
       "                                        File Content  \n",
       "0  \"Bad Together\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n...  \n",
       "1  \"Begging\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n\\n\\n\"...  \n",
       "2  \"Be The One\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n\\n...  \n",
       "3  \"Blow Your Mind (Mwah)\" lyrics\\n\\nDua Lipa Lyr...  \n",
       "4  \"Dreams\" lyrics\\n\\nDua Lipa Lyrics\\n\\n\\n\\n\\n\"D...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the path to the lyrics folder\n",
    "folder_path = data_location + lyrics_folder\n",
    "\n",
    "# List to store all file paths\n",
    "all_file_paths = []\n",
    "\n",
    "# go through all directories and subdirectories\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        all_file_paths.append(file_path)\n",
    "        \n",
    "# read the contents of each file and store them along with the filename using the folder_paths variable\n",
    "file_data = []\n",
    "for file_path in all_file_paths:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        filename = os.path.relpath(file_path, folder_path) # Get the relative path as filename\n",
    "        filename = os.path.basename(filename) # only getting the filename of whatever we pulled\n",
    "        file_data.append((filename, content))\n",
    "\n",
    "# create a dataframe with columns 'Filename' and 'File Content'\n",
    "df = pd.DataFrame(file_data, columns=['Filename', 'File Content'])\n",
    "\n",
    "# Display the dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6209fd8",
   "metadata": {},
   "source": [
    "### Reading in Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36aa25",
   "metadata": {},
   "source": [
    "Reading in the twitter files is easier. Since we only need the cher_followers_data and robynkonichiwa_followers_data, we're going to read in both and append it to each other to create one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2bb3187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AngelxoArts</td>\n",
       "      <td>Angelxo</td>\n",
       "      <td>1424055675030806529</td>\n",
       "      <td>Zacatlan, Puebla, Mexico</td>\n",
       "      <td>29</td>\n",
       "      <td>535</td>\n",
       "      <td>I love chill •Facebook / Instagram / SoundClou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>songsfornikola</td>\n",
       "      <td>johnny</td>\n",
       "      <td>1502717352575651840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>318</td>\n",
       "      <td>books, movies, music, nature &amp; TV shows. OG Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thibaud_lola</td>\n",
       "      <td>Thibaud Lola</td>\n",
       "      <td>1502407708246478852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>(Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KyleSew2112</td>\n",
       "      <td>Kyle S 🌹🇬🇧🇺🇦</td>\n",
       "      <td>3423966821</td>\n",
       "      <td>South East London</td>\n",
       "      <td>1258</td>\n",
       "      <td>3444</td>\n",
       "      <td>This Twitter profile is full of sarcasm and ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MusiFlo</td>\n",
       "      <td>MusiFlo</td>\n",
       "      <td>3324069364</td>\n",
       "      <td>Canada</td>\n",
       "      <td>470</td>\n",
       "      <td>1706</td>\n",
       "      <td>Flora Youssef - Blogger &amp; Founder Posting revi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      screen_name          name                   id  \\\n",
       "0     AngelxoArts       Angelxo  1424055675030806529   \n",
       "1  songsfornikola        johnny  1502717352575651840   \n",
       "2    thibaud_lola  Thibaud Lola  1502407708246478852   \n",
       "3     KyleSew2112  Kyle S 🌹🇬🇧🇺🇦           3423966821   \n",
       "4         MusiFlo       MusiFlo           3324069364   \n",
       "\n",
       "                   location  followers_count  friends_count  \\\n",
       "0  Zacatlan, Puebla, Mexico               29            535   \n",
       "1                       NaN                6            318   \n",
       "2                       NaN                3             69   \n",
       "3         South East London             1258           3444   \n",
       "4                    Canada              470           1706   \n",
       "\n",
       "                                         description  \n",
       "0  I love chill •Facebook / Instagram / SoundClou...  \n",
       "1  books, movies, music, nature & TV shows. OG Sw...  \n",
       "2  (Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...  \n",
       "3  This Twitter profile is full of sarcasm and ra...  \n",
       "4  Flora Youssef - Blogger & Founder Posting revi...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change working directory\n",
    "os.chdir(data_location+twitter_folder)\n",
    "\n",
    "# define filepath for robyn\n",
    "robyn_filepath = \"robynkonichiwa_followers_data.txt\"\n",
    "\n",
    "# Read in the text file as a DataFrame\n",
    "robyn_df = pd.read_csv(robyn_filepath, delimiter='\\t') \n",
    "robyn_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93702f32",
   "metadata": {},
   "source": [
    "The cher data file gave me more trouble with a row reading as 12 columns instead of 7, so i had to find a way around that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a514bd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>742153090850164742</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81</td>\n",
       "      <td>514</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1496463006451974150</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3366479914</td>\n",
       "      <td></td>\n",
       "      <td>752</td>\n",
       "      <td>556</td>\n",
       "      <td>csu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>83915043</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888</td>\n",
       "      <td>2891</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rodhandyjj</td>\n",
       "      <td>Art Vandalay 🇺🇦🇺🇦🇺🇦</td>\n",
       "      <td>1386454132022824962</td>\n",
       "      <td>Maine, USA</td>\n",
       "      <td>104</td>\n",
       "      <td>159</td>\n",
       "      <td>I’m unemployed and live with my parents. MOOPS!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    screen_name                 name                   id        location  \\\n",
       "0    horrormomy                 Jeny   742153090850164742           Earth   \n",
       "1  anju79990584                 anju  1496463006451974150                   \n",
       "2  gallionjenna                    J           3366479914                   \n",
       "3       bcscomm              bcscomm             83915043  Washington, DC   \n",
       "4    Rodhandyjj  Art Vandalay 🇺🇦🇺🇦🇺🇦  1386454132022824962      Maine, USA   \n",
       "\n",
       "  followers_count friends_count  \\\n",
       "0              81           514   \n",
       "1              13           140   \n",
       "2             752           556   \n",
       "3             888          2891   \n",
       "4             104           159   \n",
       "\n",
       "                                         description  \n",
       "0           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜  \n",
       "1          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡  \n",
       "2                                                csu  \n",
       "3  Writer @Washinformer @SpelmanCollege alumna #D...  \n",
       "4    I’m unemployed and live with my parents. MOOPS!  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define filepath for cher\n",
    "cher_filepath = \"cher_followers_data.txt\"\n",
    "\n",
    "# define column names\n",
    "columns = [\"screen_name\", \"name\", \"id\", \"location\", \"followers_count\", \"friends_count\", \"description\"]\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Open the text file and read line by line\n",
    "with open(cher_filepath, 'r', encoding = 'utf-8') as file:\n",
    "    for line in file:\n",
    "        # Split the line by tab delimiter\n",
    "        fields = line.strip().split('\\t')\n",
    "        # If the line has expected number of fields\n",
    "        if len(fields) == len(columns):\n",
    "            data.append(fields)\n",
    "\n",
    "# the list will be changed to a dataframe\n",
    "cher_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# dropping the first row\n",
    "cher_df = cher_df.iloc[1:]\n",
    "cher_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "cher_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words(\"english\")) \n",
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b327033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]\n",
       "1        [163㎝／愛かっぷ💜26歳🍒, 工〇好きな女の子💓, フォローしてくれたらdmします🧡]\n",
       "2                                                [csu]\n",
       "3    [writer, washinformer, spelmancollege, alumna,...\n",
       "4              [i’m, unemployed, live, parents, moops]\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean twitter data here\n",
    "# create a function to clean data\n",
    "def clean_data(text):\n",
    "    if isinstance(text, str):\n",
    "        new_text = ''.join(char for char in text if char not in punctuation) # getting rid of punctuation\n",
    "        \n",
    "        # fold to lowercase and split on whitespace\n",
    "        tokens = new_text.lower().split()\n",
    "        \n",
    "        # remove stopwords\n",
    "        cleaned_tokens = [token for token in tokens if token not in sw]\n",
    "        \n",
    "        return cleaned_tokens\n",
    "    else:\n",
    "        return(text)\n",
    "\n",
    "\n",
    "# apply to each cell of the dataframe for cher\n",
    "new_cher_df = cher_df['description'].apply(clean_data)\n",
    "new_cher_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4664f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [love, chill, •facebook, instagram, soundcloud...\n",
       "1    [books, movies, music, nature, tv, shows, og, ...\n",
       "2    [amauteur, en, herbe, 🌱, juriste, en, paille, ...\n",
       "3    [twitter, profile, full, sarcasm, rants, occas...\n",
       "4    [flora, youssef, blogger, founder, posting, re...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply to each cell of the dataframe for robyn\n",
    "new_robyn_df = robyn_df['description'].apply(clean_data)\n",
    "new_robyn_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean lyrics data here\n",
    "# creating a new df\n",
    "lyrics_df = df\n",
    "\n",
    "# splitting based on artist for lyrics\n",
    "dualipa_lyrics_df = df[df['Filename'].str.contains('dualipa', case = False)].reset_index()\n",
    "hozier_lyrics_df = df[df['Filename'].str.contains('hozier', case = False)].reset_index()\n",
    "\n",
    "# cleaning the data\n",
    "new_dualipa_lyrics = dualipa_lyrics_df['File Content'].apply(clean_data)\n",
    "new_hozier_lyrics = hozier_lyrics_df['File Content'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813590a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [bad, together, lyrics, dua, lipa, lyrics, bad...\n",
       "1    [begging, lyrics, dua, lipa, lyrics, begging, ...\n",
       "2    [one, lyrics, dua, lipa, lyrics, one, see, moo...\n",
       "3    [blow, mind, mwah, lyrics, dua, lipa, lyrics, ...\n",
       "4    [dreams, lyrics, dua, lipa, lyrics, dreams, la...\n",
       "Name: File Content, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dualipa_lyrics.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581046a3",
   "metadata": {},
   "source": [
    "I had to rewrite the descriptive_stats function in order to read in the nested list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f466766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_descriptive_stats(tokens, num_tokens = 5 , verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    flat_tokens = [token for sublist in tokens if isinstance(sublist, list) for token in sublist if not isinstance(token, float)]\n",
    "    total_tokens = len(flat_tokens)  # total number of tokens\n",
    "    num_tokens = len(flat_tokens) # number of tokens\n",
    "    num_unique_tokens = len(set(flat_tokens)) # number of unique tokens\n",
    "    lexical_diversity = num_unique_tokens/num_tokens  # lexical diversity\n",
    "    num_characters = sum(len(token) for token in flat_tokens) # number of characters\n",
    "    most_common_tokens = Counter(flat_tokens).most_common(num_tokens)[0:5] # most common tokens\n",
    "   \n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "        \n",
    "    print(f\"The five most common tokens are {most_common_tokens} in the data.\")      \n",
    "    \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3659 tokens in the data.\n",
      "There are 644 unique tokens in the data.\n",
      "There are 16863 characters in the data.\n",
      "The lexical diversity is 0.176 in the data.\n",
      "The five most common tokens are [('dont', 99), ('love', 88), ('im', 81), ('know', 79), ('like', 61)] in the data.\n",
      "There are 2957 tokens in the data.\n",
      "There are 942 unique tokens in the data.\n",
      "There are 14867 characters in the data.\n",
      "The lexical diversity is 0.319 in the data.\n",
      "The five most common tokens are [('power', 51), ('love', 48), ('little', 41), ('lyrics', 40), ('oh', 33)] in the data.\n",
      "There are 15670504 tokens in the data.\n",
      "There are 1517718 unique tokens in the data.\n",
      "There are 92951718 characters in the data.\n",
      "The lexical diversity is 0.097 in the data.\n",
      "The five most common tokens are [('love', 214576), ('im', 139098), ('life', 122980), ('music', 88177), ('de', 72974)] in the data.\n",
      "There are 1538163 tokens in the data.\n",
      "There are 271325 unique tokens in the data.\n",
      "There are 9397180 characters in the data.\n",
      "The lexical diversity is 0.176 in the data.\n",
      "The five most common tokens are [('music', 15160), ('love', 11683), ('im', 9052), ('och', 7922), ('life', 7387)] in the data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1538163, 271325, 0.1763954795428053, 9397180]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "\n",
    "new_descriptive_stats(new_dualipa_lyrics)\n",
    "new_descriptive_stats(new_hozier_lyrics)\n",
    "new_descriptive_stats(new_cher_df)\n",
    "new_descriptive_stats(new_robyn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: The stopwords would probably dominate the \"top 5 words\" list. This would also mean we wouldn't have gotten meaningful insights on the actual top lyrics since stop words aren't necessarily content words.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: For context, the first 2 outputs are for the lyrics and the last two outputs are for the twitter posts. I thought the lyrics would contain less lexical diversity than the twitter posts. However, it seems like the twitter posts have less diversity than the lyrics. That's surprising since there are more people contributing to twitter posts than the lyrics, as there is only one artist linked to them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcbeb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gotta make everything a str, not list\n",
    "def list_to_str(cell):\n",
    "    if isinstance(cell, float):\n",
    "        return ''\n",
    "    cell = [item for item in cell if not isinstance(item, float)] # filtering out float values\n",
    "    return ''.join(map(str,cell))\n",
    "\n",
    "# create a function to extract emojis from text\n",
    "def extract_emojis(text):\n",
    "    demojized_text = emoji.demojize(text)\n",
    "    return re.findall(r':[a-zA-Z_0-9]+:', demojized_text) # matching on regular expression to get the entirety of the emoji out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3016b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':red_heart:': 76843, ':heart_suit:': 33883, ':rainbow_flag:': 30347, ':sparkles:': 29250, ':blue_heart:': 21143, ':water_wave:': 20083, ':rainbow:': 16699, ':purple_heart:': 16388, ':United_States:': 14539, ':victory_hand:': 11797}\n"
     ]
    }
   ],
   "source": [
    "# cher tweets\n",
    "# changing the pandas dataframe variable to a str\n",
    "cher_emos = new_cher_df.apply(list_to_str)\n",
    "new_cher_emos = cher_emos.apply(extract_emojis)\n",
    "\n",
    "# flatten the list of emojis\n",
    "emojis_flat = [emoji for sublist in new_cher_emos for emoji in sublist]\n",
    "\n",
    "# count the emojis\n",
    "emoji_counts = Counter(emojis_flat)\n",
    "\n",
    "# find the ten most common emojis\n",
    "top_cher_emojis = dict(emoji_counts.most_common(10))\n",
    "\n",
    "print(top_cher_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0f0b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':red_heart:': 4645, ':rainbow_flag:': 3253, ':heart_suit:': 3113, ':sparkles:': 2224, ':rainbow:': 1408, ':blue_heart:': 806, ':victory_hand:': 791, ':purple_heart:': 735, ':musical_notes:': 698, ':black_heart:': 611}\n"
     ]
    }
   ],
   "source": [
    "# robyn tweets\n",
    "# changing the pandas dataframe variable to a str\n",
    "robyn_emos = new_robyn_df.apply(list_to_str)\n",
    "new_robyn_emos = robyn_emos.apply(extract_emojis)\n",
    "\n",
    "# flatten the list of emojis\n",
    "emojis_flat = [emoji for sublist in new_robyn_emos for emoji in sublist]\n",
    "\n",
    "# count the emojis\n",
    "emoji_counts = Counter(emojis_flat)\n",
    "\n",
    "# find the ten most common emojis\n",
    "top_robyn_emojis = dict(emoji_counts.most_common(10))\n",
    "\n",
    "print(top_robyn_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eea76ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to semi-clean data\n",
    "# the function before removed all punctuations, including hashtags\n",
    "# we want them back in \n",
    "\n",
    "# clean the raw data again except for punctuation\n",
    "def semi_clean_data(text):\n",
    "    if isinstance(text, str):\n",
    "        \n",
    "        # fold to lowercase and split on whitespace\n",
    "        tokens = text.lower().split()\n",
    "        \n",
    "        # remove stopwords\n",
    "        cleaned_tokens = [token for token in tokens if token not in sw]\n",
    "        \n",
    "        return cleaned_tokens\n",
    "    else:\n",
    "        return(text)\n",
    "    \n",
    "# find the all the hashtags\n",
    "def common_hashtags(texts):\n",
    "    \n",
    "    hashtags_count = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        hashtags = re.findall(r'#\\w+\\s*\\w*', text)\n",
    "        if hashtags:\n",
    "            hashtags_count.update(hashtags)\n",
    "    \n",
    "    return hashtags_count\n",
    "\n",
    "# count the hashtags that are the same in the counter\n",
    "\n",
    "def count_hashtags(hashtags_counters):\n",
    "    hashtag_counts = defaultdict(int)\n",
    "    for counter in hashtags_counters:\n",
    "        for hashtag, count in counter.items():\n",
    "            hashtag_counts[hashtag] += count\n",
    "    return dict(hashtag_counts)\n",
    "\n",
    "# gotta change the panda series to strings\n",
    "\n",
    "def list_to_str(cell):\n",
    "    if isinstance(cell, float):\n",
    "        return ''\n",
    "    cell = [item for item in cell if not isinstance(item, float)] # filtering out float values\n",
    "    return ''.join(map(str,cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52aad116",
   "metadata": {},
   "outputs": [],
   "source": [
    "cherhash_data = cher_df['description'].apply(semi_clean_data)\n",
    "robynhash_data = robyn_df['description'].apply(semi_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3460e9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, &, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]\n",
       "1        [163㎝／愛かっぷ💜26歳🍒, 工〇好きな女の子💓, フォローしてくれたらdmします🧡]\n",
       "2                                                [csu]\n",
       "3    [writer, @washinformer, @spelmancollege, alumn...\n",
       "4            [i’m, unemployed, live, parents., moops!]\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cherhash_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07c396f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ten most common hashtags by Cher Followers:\n",
      "[('#resist', 10720), ('#blm', 9598), ('#blacklivesmatter', 7374), ('#theresistance', 3260), ('#fbr', 3096), ('#resistance', 2763), ('#voteblue', 2169), ('#lgbtq', 1763), ('#music', 1402), ('#bluewave', 1399)]\n"
     ]
    }
   ],
   "source": [
    "# getting the cher hashtags\n",
    "cherhash_data= cherhash_data.apply(list_to_str)\n",
    "cher_hashtags = cherhash_data.apply(lambda x: common_hashtags([x]))\n",
    "total_hashtags = count_hashtags(cher_hashtags)\n",
    "\n",
    "top_10_cher = sorted(total_hashtags.items(), key=lambda x: x[1], reverse=True)\n",
    "top_10_cher = top_10_cher[:10]\n",
    "print(\"\\nTen most common hashtags by Cher Followers:\")\n",
    "print(top_10_cher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01b83ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ten most common hashtags by Robyn Followers:\n",
      "[('#blacklivesmatter', 530), ('#blm', 327), ('#music', 268), ('#teamfollowback', 123), ('#edm', 84), ('#resist', 80), ('#lgbtq', 75), ('#travel', 67), ('#art', 65), ('#freebritney', 61)]\n"
     ]
    }
   ],
   "source": [
    "# getting the robyn hashtags\n",
    "robynhash_data= robynhash_data.apply(list_to_str)\n",
    "robyn_hashtags = robynhash_data.apply(lambda x: common_hashtags([x]))\n",
    "total_hashtags = count_hashtags(robyn_hashtags)\n",
    "\n",
    "top_10_robyn = sorted(total_hashtags.items(), key=lambda x: x[1], reverse=True)\n",
    "top_10_robyn = top_10_robyn[:10]\n",
    "print(\"\\nTen most common hashtags by Robyn Followers:\")\n",
    "print(top_10_robyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most common words in song titles by dualipa are:\n",
      "{'new': 3, 'acoustic': 2, 'live': 2, 'rules': 2, 'bad': 1}\n",
      "The five most common words in song titles by hozier are\n",
      "{'angel': 1, 'small': 1, 'death': 1, 'codeine': 1, 'scene': 1}\n"
     ]
    }
   ],
   "source": [
    "# function to extract content within the first pair of quotes\n",
    "def extract_content(text):\n",
    "    match = re.search(r'\"([^\"]*)\"', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# apply extract_content to the \"File Content\" column\n",
    "dualipa_titles = dualipa_lyrics_df['File Content'].apply(extract_content)\n",
    "hozier_titles = hozier_lyrics_df['File Content'].apply(extract_content)\n",
    "\n",
    "#clean and tokenize data for dualipa\n",
    "dualipa_clean = dualipa_titles.apply(clean_data)\n",
    "\n",
    "# flatten the list of title words\n",
    "titles_flat = [word for sublist in dualipa_clean for word in sublist]\n",
    "\n",
    "# count the emojis\n",
    "title_word_counts = Counter(titles_flat)\n",
    "\n",
    "# find the ten most common emojis\n",
    "top_dualipa_words = dict(title_word_counts.most_common(5))\n",
    "\n",
    "print(\"The five most common words in song titles by dualipa are:\")\n",
    "print(top_dualipa_words)\n",
    "\n",
    "#clean and tokenize data for hozier\n",
    "hozier_clean = hozier_titles.apply(clean_data)\n",
    "\n",
    "# flatten the list of title words\n",
    "titles_flat = [word for sublist in hozier_clean for word in sublist]\n",
    "\n",
    "# count the emojis\n",
    "title_word_counts = Counter(titles_flat)\n",
    "\n",
    "# find the ten most common emojis\n",
    "top_hozier_words = dict(title_word_counts.most_common(5))\n",
    "\n",
    "print(\"The five most common words in song titles by hozier are\")\n",
    "print(top_hozier_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: That regular expression matches the string with one or more whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "new_dualipa = pd.DataFrame()\n",
    "\n",
    "# Repeat \"dualipa\" 20 times and assign it to a new column\n",
    "new_dualipa['Artist'] = pd.Series(['dualipa'] * 20)\n",
    "\n",
    "# tokzenizing the lyrics and storing it\n",
    "new_dualipa['Tokenized'] = dualipa_lyrics_df['File Content'].apply(tokenize_lyrics)\n",
    "\n",
    "# count the lyrics and store it\n",
    "new_dualipa['Tokenized Count'] = new_dualipa['Tokenized'].apply(len)\n",
    "\n",
    "# new df for hozier\n",
    "new_hozier = pd.DataFrame()\n",
    "\n",
    "# Repeat \"dualipa\" 20 times and assign it to a new column\n",
    "new_hozier['Artist'] = pd.Series(['hozier'] * 20)\n",
    "\n",
    "# tokzenizing the lyrics and storing it\n",
    "new_hozier['Tokenized'] = hozier_lyrics_df['File Content'].apply(tokenize_lyrics)\n",
    "\n",
    "# count the lyrics and store it\n",
    "new_hozier['Tokenized Count'] = new_hozier['Tokenized'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6787f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([new_dualipa,new_hozier], ignore_index = True)\n",
    "combined_df = combined_df.rename(columns = {'Tokenized Count':'Length'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6440b37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of song lengths by artist')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df.groupby('Artist')['Length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)\n",
    "plt.title('Histogram of song lengths by artist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
